---
title: "Modeling Protocol"
author: "Caroline Newell"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

Constructed so I can search "LongfinSmelt"and replace with Gensp name and have the code run.

### Loading libraries and data

```{r}
library(readr)
library(dismo)
library(caret)
library(here)
library(cowplot)
library(Hmisc)
library(tidyverse)

PresAbs<-read_csv("C:/Users/cnewe/OneDrive/Documents/Incubator/Code/FishForesightIncubator/Data/SMFS_PresAbs.csv")
```

# PresenceAbsence

## Step 1: Data visualization, Quality Check

Before modeling, we will ensure the data is up to snuff.

First things first, we need to get the data for the LongfinSmelt of interest we want to model.

```{r}
LongfinSmeltData<-PresAbs %>% filter(gensp %in% "Spirinchus thaleichthys")
LongfinSmeltData<-as.data.frame(LongfinSmeltData)
```

Now lets check the dataset.

```{r}
glimpse(LongfinSmeltData)
summary(LongfinSmeltData)
```

Let's make a couple of plots for making predictions.

```{r}
Plot_Water<-LongfinSmeltData %>% ggplot(aes(y=PresAbs, x=WaterTemperature, color = TowDuration)) + geom_violin(fill = "lightseagreen")+ stat_summary(fun.data="mean_sdl", fun.args=list(mult=1), geom= "pointrange", color="black") + theme_bw() + ylab("") + xlab("Water Temperature (C)")

Plot_DO<-LongfinSmeltData %>% ggplot(aes(y=PresAbs, x=DO, color = TowDuration)) + 
  geom_violin(fill = "lightseagreen")+ stat_summary(fun.data="mean_sdl", fun.args=list(mult=1), geom= "pointrange", color="black") + theme_bw()+ ylab("")+ xlab("Dissolved Oxygen (mg/L)")

Plot_Secchi<-LongfinSmeltData %>% ggplot(aes(y=PresAbs, x=Secchi, color = TowDuration)) + 
  geom_violin(fill = "lightseagreen")+ stat_summary(fun.data="mean_sdl", fun.args=list(mult=1), geom= "pointrange", color="black") + theme_bw()+ ylab("") + xlab("Secchi (cm)")

Plot_Salinity<-LongfinSmeltData %>% ggplot(aes(y=PresAbs, x=Salinity, color = TowDuration)) + geom_violin(fill = "lightseagreen")+ stat_summary(fun.data="mean_sdl", fun.args=list(mult=1), geom= "pointrange", color="black") + theme_bw()+ ylab("") + xlab("Salinity")

Plots<-plot_grid(Plot_Water, Plot_DO, Plot_Secchi, Plot_Salinity, ncol = 2, labels = "AUTO")

# now add the title
title <- ggdraw() + draw_label("Longfin smelt presence/absence by water quality variables for SMFS 2011-2023", fontface = 'bold', x = 0, hjust = 0) +  theme(
    # add margin on the left of the drawing canvas,
    # so title is aligned with left edge of first plot
    plot.margin = margin(0, 0, 0, 7))

plot_grid(title, Plots, ncol = 1,
  # rel_heights values control vertical title margins
  rel_heights = c(0.1, 1))

SummTable <- LongfinSmeltData %>% 
  group_by(PresAbs) %>% 
  dplyr::summarize(
    Mean_WT = mean(WaterTemperature),
    SD_WT = sd(WaterTemperature),
    MinWT=min(WaterTemperature),
    MaxWT=max(WaterTemperature),
    Mean_DO = mean(DO),
    SD_DO = sd(DO),
    MinDO=min(DO),
    MaxDO=max(DO),
    Mean_Secchi = mean(Secchi),
    SD_Secchi = sd(Secchi),
    MinSecchi=min(Secchi),
    MaxSecchi=max(Secchi),
    Mean_Salinity = mean(Salinity),
    SD_Salinity = sd(Salinity),
    MinSal=min(Salinity),
    MaxSal=max(Salinity)
  ) 

print(SummTable)
```

Plot analysis: Average water temperature when LFS absent is 16.37 C (standard deviation is 4.64), present is 17.36 C (standard deviation 3.14). Average dissolved oxygen when LFS absent is 6.95 mg/L (sd 1.64), present is 7.39 (sd 1.18). Average secchi when LFS absent is 30.5 cm (sd 11.55), present is 30.56 cm (sd 10.18). Average salinity when LFS absent is 4.66 psu (sd 3.4) and present is 5.4 (sd 3.24).

Range for presences: 7.6 to 23.8 C; 4.2 to 10.10 mg/L; 11 to 73 cm; 0.2 to 14.2 psu.

Range for absences (other samples): 4.6 to 27 C; 1 to 13.74 mg/L; 7 to 99; 0 to 15 psu.

Most restricted presence compared to absence is dissolved oxygen. High secchi depth (clear water) also lacks presence above 75 cm.

Do the most influential?

## Step 2: Use caret package to set hyperparameters

Thanks Nima! "Rather than just randomly choosing numbers that work and seeing how well the model predicts over and over again, the caret package has built in functionality to streamline the tuning of the parameters to help you evaluate the effect of tuning the hyperparameters on predictive performance and choose the “optimal” model across these hyperparameters.

What the caret package does is create a grid of all the parameters and values you want to test. It will then some sort of validation method (e.g. k-fold cross validation), calculate the performance for each iteration of the validation, determine the optimal parameter set.

FYI: caret doesn’t using the dismo package but uses the gbm package for BRTs which is what the dismo packages is using inherently as well."

```{r}
gbmGrid <- expand.grid(interaction.depth = c(1, 2,3,4,5), #tree complexity. interaction.depth = 1 : additive model, interaction.depth = 2 : two-way interactions, etc. As each split increases the total number of nodes by 3 and number of terminal nodes by 2, the total number of nodes in the tree will be 3∗N+1 and the number of terminal nodes 2∗N+1 https://www.listendata.com/2015/07/gbm-boosted-models-tuning-parameters.html
                       n.trees = c(1000,2000, 3000), #number of trees. Minimum is 1,000 according to Elith. May upddate after running gbm.
                       shrinkage = seq(0.001, 0.01, length.out =5), #learning rate. Slower learning rate (clsoer to 0) is better according to elith.
                       #bag.fraction = 0.6, #bag fraction (proportion of observations used in selecting variables) and recommended by Elith. function won't run if I uncomment bagfraction... Elith says stochasticity improves model preformance and fractions in the range 0.5-0.75 have given the best results for p-a responses.
                       n.minobsinnode = 10 # the minimum number of observations in trees' terminal nodes. Tells it when to stop. Also, controls the complexity of each tree. Since we tend to use shorter trees this rarely has a large impact on performance. Typical values range from 5–15 where higher values help prevent a model from learning relationships which might be highly specific to the particular sample selected for a tree (overfitting) but smaller values can help with imbalanced target classes in classification problems. https://bradleyboehmke.github.io/HOML/gbm.html 10 is what gbm function sets it to. Ie the minimum number of training set samples in a node to commence splitting . honestly dont worry about this. https://stats.stackexchange.com/questions/30645/role-of-n-minobsinnode-parameter-of-gbm-in-r
                       )

gbmGrid %>% head() #take a look at what we just made
```

```{r}
#specify type of resampling
fitControl <- trainControl(## 10-fold CV
                           method = "cv",
                           number = 10, #Can increase! Either the number of folds or number of resampling iterations
                           classProbs = TRUE, ## Estimate class probabilities for classification models
                           summaryFunction = twoClassSummary #a function to compute performance metrics across resamples. 
                           )
```

Here we do a 10 fold cross validation (again to decrease time but feel free to increase it). Specifying ‘twoClassSummary’ will compute the sensitivity (true positive rate), specificity (true negative rate), and the area under the ROC curve (also called AUC which is a metric of predictive performance….the closer to 1 the better, 0.5 no better than random, closer to 0 poor at predicting). Nima likes these metrics personally as he finds them intuitive and used widely in the literature. The default metrics are kappa and accurate for presence/absence data and for more numerical responses, like counts or abundance, it will be root mean squared error.

Now we can use the `train` function to run the analysis.

```{r}
set.seed(124) #setting the seed for reproducibility. BRT have randomness in them so if you want to make sure you are getting the same result each time make sure to set the seed

system.time(gbmFit <- train(PresAbs ~ WaterTemperature + Secchi + DO + Salinity + TowDuration, #caret wants non-numeric presence, absence. Might need to change it to Yes and No.
                data = LongfinSmeltData, 
                method = "gbm", 
                trControl = fitControl, 
                verbose = FALSE, #for gbm's this is needed.
                ## Now specify the exact models to evaluate:
                tuneGrid = gbmGrid, #what we made above
                metric = "ROC" 
                ))
#run time 6 min for first model, 10 for second, 10.25 for third.

gbmFit 
# save the model
saveRDS(gbmFit, here("data", "LFS_brt_gbm_fit.rds"))

# we can plot the results too which will be easier to interpret
ggplot(gbmFit)
```

**1st model run:**

|         | interaction.depth | n.trees | shrinkage                  | n.minobsinnode |
|--------------|--------------|--------------|----------------|--------------|
| Options | 3,5,7             | 2000    | seq(0.01,0.5,length.out=5) | 10             |
| Best    | 3                 | 2000    | 0.01                       | 10             |

The best model from the first run had the following results:

| ROC    | Sensitivity | Specificity |
|--------|-------------|-------------|
| 0.8204 | 0.9919      | 0.10303     |

For the second run I played around with adding more options.

**2nd model run:**

|         | interaction.depth | n.trees          | shrinkage                  | n.minobsinnode |
|---------------|---------------|---------------|---------------|---------------|
| Options | 3,5,7             | 1000, 2000, 3000 | seq(0.01,0.5,length.out=5) | 10, 20, 30     |
| Best    | 5                 | 1000             | 0.01                       | 30             |

: The best model from the second run had the following results:

| ROC    | Sensitivity | Specificity |
|--------|-------------|-------------|
| 0.8271 | 0.9909      | 0.10368     |

: Now I will focus in on those best values of the second run to see if it can get even better.

**3rd model run:**

|         | interaction.depth | n.trees        | shrinkage                    | n.minobsinnode |
|--------------|---------------|---------------|---------------|---------------|
| Options | 4,5,6             | 500, 700, 1000 | seq(0.001,0.01,length.out=5) | 10, 30, 50     |
| Best    | 6                 | 500            | 0.0055                       | 50             |

: The best model from the third run had the following results:

| ROC    | Sensitivity | Specificity |
|--------|-------------|-------------|
| 0.8384 | 0.994       | 0.0935      |

: I'm not trustful of this trend for increasing n.min and decreasing number of trees. Elith et al. recommends at least 1000 trees. The gbm package defaults at using 10 for n.min. A perfect ROC AUC isn't necessarily the best option... So what results do I get if I run the train with defaults suggested by Elith?

**4th model run:**

|         | interaction.depth | n.trees          | shrinkage                    | n.minobsinnode |
|---------------|---------------|---------------|---------------|---------------|
| Options | 2,3,4,5           | 1000, 2000, 3000 | seq(0.001,0.01,length.out=5) | 10             |
| Best    | 5                 | 1000             | 0.00325                      | 10             |

: The best model from the fourth run had the following results:

| ROC       | Sensitivity | Specificity |
|-----------|-------------|-------------|
| 0.8274461 | 0.9934773   | 0.10779     |

: Is this good?

Sensitivity = True positives Specificity = True negatives

Evaluate model performance through predictive deviance. Don't want it too high or too low. As hyper parameters are changed then that may shift optimal options for others.

Nima: "Additionally, I will note that the more folds, parameters, and values you look that the longer this evaluation will take. I usually just look at different learning rate, number of trees, and tree complexity and I look between 3 - 5 values for each. If you end up using gbm.step to build your model, which I will talk about below, you don’t need to look at number of trees because that function will figure that out. My collaborators and I usually keep bag.fraction as 0.6 and I have never touched n.minobsinnode."

I think I will just use gbm package from dismo and look at predictive deviance instead of ROC.

## Step 3: Use gbm() from dismo to run model and create outputs

Thanks Nima and Elith! Follow <https://bradleyboehmke.github.io/HOML/gbm.html> for tuning strategy?

```{r}
 set.seed(124) # for reproducibility 
#gbm.step wants bernoulli response {0, 1} 
system.time( brt1 <- dismo::gbm.step(data=LongfinSmeltData, 
                         gbm.x= c(21, 22, 23, 25, 29), # environmental variables
                         gbm.y= 17, # response variable
                         family = "bernoulli", # for counts this would be "poisson"
                         tree.complexity = 5, # complexity of the interactions that the model will fit. 5-way interaction since these may all interact with one another? Typical values range from a depth of 3–8 but it is not uncommon to see a tree depth of 1 (J. Friedman, Hastie, and Tibshirani 2001). Smaller depth trees such as decision stumps are computationally efficient (but require more trees); however, higher depth trees allow the algorithm to capture unique interactions but also increase the risk of over-fitting. Note that larger  n  or  p training data sets are more tolerable to deeper trees.
                         learning.rate = 0.005,  # optimized to end up with >1000 trees. Smaller values make the model robust to the specific characteristics of each individual tree, thus allowing it to generalize well. Smaller values also make it easier to stop prior to overfitting; however, they increase the risk of not reaching the optimum with a fixed number of trees and are more computationally demanding
                         bag.fraction = 0.75 # default for Elith, amount of input data used each time
                         ))

# make sure to save the model!
summary(brt1)

names(brt1) #tells us the components of output

# To pull out one component of the list, use a number (angaus.tc5.lr01[[29]]) or name (angaus.tc5.lr01$cv.statistics); but be careful - some are as big as the dataset! e.g. there will be 1000 fitted values - find this by typing length(angaus.tc5.lr01$fitted)

 # reponse curves
par(mar=c(2, 2, 1, 1))

dismo::gbm.plot(brt1, n.plots = 5, write.title= T, rug = T, smooth = TRUE, plot.layout=c(2,3), common.scale = T)

# save the model
saveRDS(brt1, here("data", "LongfinSmelt_brt_gbm_step.rds"))

```

-   tree.complexity = 5, learning.rate = 0.001, bag.fraction = 0.6 (Nima's preference) fit final gbm model with a fixed number of 5800 trees for Binary mean total deviance = 0.467 mean residual deviance = 0.304

estimated cv deviance = 0.377 ; se = 0.008

training data correlation = 0.546 cv correlation = 0.359 ; se = 0.024

training data AUC score = 0.921 cv AUC score = 0.828 ; se = 0.014

elapsed time - 0.03 minutes user system elapsed 92.58 0.03 105.67 - Increase bag fraction to the recommendation from Elith et al of 0.75: fit final gbm model with a fixed number of 6150 trees for Binary

mean total deviance = 0.467 mean residual deviance = 0.302

estimated cv deviance = 0.373 ; se = 0.01

training data correlation = 0.552 cv correlation = 0.368 ; se = 0.026

training data AUC score = 0.923 cv AUC score = 0.831 ; se = 0.012

elapsed time - 0.03 minutes user system elapsed 88.67 0.14 110.60

-   learning rate 0.05 attempted but too big. Has to be smaller to reach minimum

-   learning rate 0.005: fit final gbm model with a fixed number of 1500 trees for Binary

mean total deviance = 0.467 mean residual deviance = 0.291

estimated cv deviance = 0.377 ; se = 0.014

training data correlation = 0.574 cv correlation = 0.36 ; se = 0.04

training data AUC score = 0.93 cv AUC score = 0.835 ; se = 0.015

elapsed time - 0.46 minutes user system elapsed 20.75 0.05 27.56

Firstly, the things you can see: The R console will show some results (see the Word Working Guide of the Elith tutorial for an example). It reports a brief model summary & all the values are also retained in the model object, so they will be permanently kept (as long as you save the R workspace before quitting).

There will also be a graph.. This model was built with the default 10-fold cross-validation (CV). The solid black curve is the mean, and the dotted curves +- 1 standard error, for the changes in predictive deviance (ie as measured on the excluded folds of the CV). The red line shows the minimum of the mean, and the green line the number of trees at which that occurs. The final model that is returned in the model object is built on the full data set, using the number of trees identified as optimal.

## Validation

Below are 2 validation schemes that we typically use to assess how well the model is predicting. The first is a K-fold cross-validation where the data is split in k number of equal sections (i.e. folds). A model is the trained on one fold and then its predicts to the rest of the folds. It then iterates this process through all the folds. Most studies do 10 folds as this has shown to be a good number. Too low or too high has shown to potentially bias / cause variability in results.

Performance metrics that are calculated are deviance explained, AUC, and TSS. Deviance explained is a metric for how much variance did our model explain (you can think of is as R squared) and AUC and TSS are assessing predictive skill. Since you are using abundance data you may need to change that to RMSE or MAE (I’d suggest taking a look at the Metrics package for that)

NOTE: whether you are using gbm.fixed or gbm.step to fit your BRTS, you will have to make a slight change to this function below. I’ve commented lines out since I made the model above using gbm.fixed but if used gbm.step I would used the code on those lines instead.

```{r}
#k fold cross validation function

eval_kfold_brt <- function(dataInput, gbm.x, gbm.y, learning.rate = 0.005, k_folds = 10, tree.complexity = 5, bag.fraction = 0.75){
  dataInput$Kset <- dismo::kfold(dataInput, k_folds) #randomly allocate k groups
  Evaluations_kfold_BRT <- as.data.frame(matrix(data=0,nrow=5,ncol=4)) 
  colnames(Evaluations_kfold_BRT) <- c("k","Deviance","AUC","TSS")
  counter=1
  for (k in 1:k_folds){
    print(k)
    train <- dataInput[dataInput$Kset!=k,]
    test <- dataInput[dataInput$Kset==k,]
    
   # brt.k <- dismo::gbm.fixed(data=train, gbm.x= gbm.x, 
                            # gbm.y = gbm.y, 
                           #  family="bernoulli", 
                           #  tree.complexity = tree.complexity,
                          #   learning.rate = learning.rate, 
                          #   bag.fraction = bag.fraction,
                          #   n.trees = n.trees)
    
     brt.k <- dismo::gbm.step(data=train, gbm.x= gbm.x, 
                             gbm.y = gbm.y, 
                              family="bernoulli", 
                             tree.complexity = tree.complexity,
                             learning.rate = learning.rate, 
                             bag.fraction = bag.fraction)
    
    preds <- gbm::predict.gbm(brt.k, test,
                              n.trees=brt.k$gbm.call$best.trees, 
                              type="response")
    
    dev_eval3<-function(x){
     # null <- x$self.statistics$null.deviance #use with gbm.fixed
     # res <- x$self.statistics$resid.deviance #use with gbm.fixed
      null <- x$self.statistics$mean.null #use with gbm.step
      res <- x$self.statistics$mean.resid #use with gbm.step
      dev=((null - res)/null)*100
      return(dev)
    }
    dev<-dev_eval3(brt.k)
    d <- cbind(test[gbm.y], preds)
    pres <- as.numeric(d[d[,1]==1,2])
    abs <- as.numeric(d[d[,1]==0,2])
    e <- dismo::evaluate(p=pres, a=abs)
    Evaluations_kfold_BRT[counter,1] <- k
    Evaluations_kfold_BRT[counter,2] <- dev
    Evaluations_kfold_BRT[counter,3] <- e@auc
    Evaluations_kfold_BRT[counter,4] <- max(e@TPR + e@TNR-1)
    counter=counter+1 
  }
  return(Evaluations_kfold_BRT)
}
  
  
brt_SDM_k <- eval_kfold_brt(dataInput = LongfinSmeltData,
                        gbm.x = c(21, 22, 23, 25, 29),
                        gbm.y=17, learning.rate = 0.005,
                        bag.fraction = 0.75, tree.complexity = 5,
                        k_folds = 10)
brt_SDM_k
```

The other validation secheme that we do is a Leave one [year] out cross-validation. Just like 5 folds but now were trained the model on all the data but one year. And then we predict to that one year that was not included. This give us a sense out how well can we predict year by year as there may be some environmental variability over time.

NOTE: Same as above of changing the code whether you are using gbm.fixed or gbm.step

```{r}
#leave one year out function

eval_loo_brt <- function(pres, abs, gbm.x, gbm.y, learning.rate = 0.005, tree.complexity = 5, bag.fraction = 0.75){
  
  pres$year <- pres$Year
  abs$year <- abs$Year
  
  ## setup output df
  Evaluations_LOO_BRT <- as.data.frame(matrix(data = 0, nrow = 1, ncol = 5))
  colnames(Evaluations_LOO_BRT) <- c("k","Deviance","AUC","TSS","n_pres")
  counter=1
  
  u_years <- unique(pres$year)
  for (y in u_years){
    
    print(paste('Running ', which(u_years == y), ' of ', length(u_years), sep=''), sep='')
    
    pres_train <- pres[which(pres$year != y),]
    ## absences are sampled from the full input absence df
    abs_train <- abs[sample(which(abs$year != y), size = nrow(pres_train), replace=F),]
    train <- rbind(pres_train, abs_train)
    
    pres_test <- pres[which(pres$year == y),]
    abs_test <- abs[sample(which(abs$year == y), size = nrow(pres_test), replace=F),]
    test <- rbind(pres_test, abs_test)
    
#    brt.loo <- dismo::gbm.fixed(data=train, gbm.x= gbm.x, 
  #                         gbm.y = gbm.y, 
   #                        family="bernoulli", 
   #                        tree.complexity = tree.complexity,
  #                         learning.rate = learning.rate, 
   #                        bag.fraction = bag.fraction,
   #                        n.trees = n.trees)
    
     brt.loo <- dismo::gbm.step(data=train, gbm.x= gbm.x, 
                                gbm.y = gbm.y, 
                                family="bernoulli", 
                                tree.complexity = tree.complexity,
                               learning.rate = learning.rate, 
                               bag.fraction = bag.fraction)
    
    ## make predictions for eval
    preds <- gbm::predict.gbm(brt.loo, test,
                              n.trees=brt.loo$gbm.call$best.trees, 
                              type="response")
    
    dev_eval3<-function(x){
      #null <- x$self.statistics$null.deviance #use with gbm.fixed
      #res <- x$self.statistics$resid.deviance #use with gbm.fixed
      null <- x$self.statistics$mean.null #use with gbm.step
      res <- x$self.statistics$mean.resid #use with gbm.step
      dev=((null - res)/null)*100
      return(dev)
    }
    dev<-dev_eval3(brt.loo)
    d <- cbind(test[,gbm.y], preds)
    pres_y <- as.numeric(d[d[,1] == 1,2])
    abs_y <- as.numeric(d[d[,1] == 0,2])
    e <- dismo::evaluate(p = pres_y, a = abs_y)
    
    Evaluations_LOO_BRT[counter,1] <- y
    Evaluations_LOO_BRT[counter,2] <- dev
    Evaluations_LOO_BRT[counter,3] <- e@auc
    Evaluations_LOO_BRT[counter,4] <- max(e@TPR + e@TNR-1)
    Evaluations_LOO_BRT[counter,5] <- length(which(train[,gbm.y] == 1))
    
    counter=counter+1 
  }
  return(Evaluations_LOO_BRT)
}

brt_SDM_loo <- eval_loo_brt(pres = LongfinSmeltData[which(LongfinSmeltData$Binary == 1),],
                        abs = LongfinSmeltData[which(LongfinSmeltData$Binary == 0),],
                        gbm.x = c(21, 22, 23, 25, 29),
                        gbm.y=17, learning.rate = 0.005,
                        bag.fraction = 0.75, tree.complexity = 5)
brt_SDM_loo

#combining the model output, and both validation results into a list to save
eval <- list(brt = brt1, brt_k = brt_SDM_k, brt_loo = brt_SDM_loo)

#now save it

saveRDS(eval, here("data","LongfinSmelt_brt_SDM_eval_gbm_step.rds"))
```

## Analyzing Response Curves & Variable Importance

Look at Nima's code for pretty plots: <https://github.com/nfarchadi/heatwave_impacts_on_fisheries/blob/main/scripts/4_plots/FS3_FS4_response_relimport_plots.R>

```{r}
# Fig. S3. The relative importance of drivers in distribution models
# Fig. S4. Response curves of each environmental variable from distribution models


###----response plots----####

brt_LongfinSmelt <- here("data","LongfinSmelt_brt_SDM_eval_gbm_step.rds") %>% readRDS()


# get the matrix out of the `plot.gbm`
var<-brt_LongfinSmelt$brt$var.names
response_data<-data.frame()

for (i in 1:length(var)){
response_plot_data <- gbm::plot.gbm(brt_LongfinSmelt$brt,
                                 i.var = var[i],
                                 return.grid = TRUE)

response_plot_data<-response_plot_data %>% gather("variable","x",-y)

response_data<-rbind(response_data, response_plot_data)

}

response_data

response_data <-response_data %>% 
  mutate(variable = case_when(variable == "SST" ~ "SST",
                              variable == "SST_SD" ~ "SST_sd",
                              variable == "SSH" ~ "SSH",
                              variable == "SSH_SD" ~ "SSH_sd",
                              variable == "z.1" ~ "bathy",
                              variable == "z_SD" ~ "rugosity",
                              variable == "dist_port" ~ "dis_port",
                              variable == "dis_seamount" ~ "dis_seamount",
                              variable == "lunar" ~ "lunar",
                              variable == "n2" ~ "N2"),
         Fleet = "Longline") #Need to update...


response_data %>%
  ggplot() + 
  geom_line(aes(x=x, y=y, color = Fleet), size =1) + 
  facet_wrap(~variable, scales = "free", nrow = 5) +
  labs(x = "Variable Values",
       y = "Marginal Effect")+
  theme_bw() + 
  theme(panel.spacing = unit(.30, "lines"),
        strip.text = element_text(size=10),
        strip.background = element_blank(),
        strip.text.x = element_text(margin = margin(0,0,.05,0, "cm")))+
  scale_color_manual(values=c("lightseagreen"))
  
ggsave(here("Plots","LongfinSmelt_response_plots.png"),
       width = 7, height = 6, units = "in", dpi = 300)
ggsave(here("Plots","LongfinSmelt_response_plots.svg"),
       width = 7, height = 6, units = "in", dpi = 300)

###----variable contribution----####
rel.inf<-data.frame(Variable = brt_LongfinSmelt$brt$contributions$var,
                        rel.inf=brt_LongfinSmelt$brt$contributions$rel.inf) %>% 
  mutate(Variable = case_when(Variable == "SST" ~ "SST",
                              Variable == "SST_SD" ~ "SST_sd",
                              Variable == "SSH" ~ "SSH",
                              Variable == "SSH_SD" ~ "SSH_sd",
                              Variable == "z.1" ~ "bathy",
                              Variable == "oldname" ~ "newname",
                              Variable == "dist_port" ~ "dis_port",
                              Variable == "dis_seamount" ~ "dis_seamount",
                              Variable == "lunar" ~ "lunar",
                              Variable == "n2" ~ "N2"),
         Fleet = "Longline")


rel.inf %>% 
  ggplot()+
  geom_bar(aes(x = Variable,y=rel.inf, fill = Fleet),
           stat="identity", position=position_dodge())+
  scale_fill_manual(values=c("lightseagreen")) +
  theme_bw() +
  labs(x = "Environmental Variable",
       y = "Relative Importance (%)")+
  theme(axis.text.x = element_text(angle = 45,hjust=1))

ggsave(here("Plots","FS3_relative_importance.png"),
       width = 7, height = 6, units = "in", dpi = 300)
ggsave(here("Plots","FS3_relative_importance.svg"),
       width = 7, height = 6, units = "in", dpi = 300)
```

# NOTES

-   If number of trees below 1k, need to lower learning rate.
-   There may be a difference in the caret output and gbm output (5%). To avoid this can use the default grid to optimize parameters and use predict to have the same results... Or don't worry about it.
