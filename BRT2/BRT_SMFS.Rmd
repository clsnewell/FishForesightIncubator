 ---
title: "BRT_SMFS"
author: "Caroline Newell"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

CANT USE POISSON ON ZEROS - POSITIVE NUMBERS ONLY I try to model species abundance data using the gbm.step function in R by Elith et al. 2008. In this function you have to choose a family aka loss function. There are several options:

-   "gaussian" (for minimizing squared error)

-   "laplace" (for minimizing absolute loss)

-   "bernoulli" (logistic regression for 0-1 out-comes)

-   "poisson" (count outcomes; requires the response to be a positive integer)

    <https://stats.stackexchange.com/questions/257946/how-to-choose-the-family-in-gbm-step>

    In gradient boosting the trees (or whatever weak learners) are not fit to the loss function (the function being minimized), they are fit to the gradient of the loss function with respect to the prediction (\*).

That's hard to digest, so here's how it works out in the squared error case. Here the loss function for a single data point is

$L(y,yhat)=(y−yhat)^2$

The yhat is what I'm calling the prediction. If we treat it like a formal variable and differentiate, we get

$∇L(y,yhat)=−2(y−yhat)$

It is this function that the trees are fit to, we plug the appropriate values of y and yhat into ∇L , and the result is our working response for one round of boosting. In the initial boosting iteration we initialize y\^ to a constant (here the mean of the response), and in subsequent iterations to the prediction of the previous rounds of boosting. <https://stats.stackexchange.com/questions/217416/gbm-impact-of-the-loss-function>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading Libraries

```{r}
library(dismo)
library(tidyverse)
library(lubridate)
library(caret)
library(sf)
library(readr)
setwd("C:/Users/cnewe/OneDrive/Documents/Incubator/Code/FishForesightIncubator")
```

# Loading Data

```{r}
SMFS <- read_csv("Data/SMFS.csv")
```

# TP Practice

```{r}
glimpse(SMFS)
#Lets build a df with just TP data.
TPDataRaw<-SMFS %>% filter(OrganismCode %in% "TP") 
TPDataRaw<-as.data.frame(TPDataRaw)

#make this a function or for loop so we can do multiple species at once?
#TPData$CPUE<-as.numeric(TPData$CPUE)
glimpse(TPDataRaw)
summary(TPDataRaw)
```

7,588 trawls in this dataset.

I want to simplify the dataframe a bit to just include columns of interest: CPUE, WaterTemperature, Secchi, DO, Salinity

There are lots of columns I am interested in. For now I only want the bare minimum just to get a feel for these BRTs.

```{r}
TPData<- TPDataRaw %>% select(Count, WaterTemperature, Secchi, DO, Salinity, TowDuration)

glimpse(TPData)



#MakeTPData Pres/Abs and try again. Then see if there is improvement. 
TPData_Binary<-TPData %>% mutate(PresAbs = if_else(Count > 0, "present", "absent"))

TPData_Binary<-TPData_Binary %>% mutate(PresAbs2 = if_else(PresAbs %in% "present", 1, 0))
# caret package is weird and doesnt like integers (0 & 1) for presence absence data so I have to convert it to a factor


#Some simple plotting
TP_Water<-TPData_Binary %>% ggplot(aes(y=Count, x=WaterTemperature, color = TowDuration)) + 
  geom_point()+ theme_bw()
#tow duration not reflected

TP_DO<-TPData_Binary %>% ggplot(aes(y=Count, x=DO, color = TowDuration)) + 
  geom_point()+ theme_bw()

TP_Secchi<-TPData_Binary %>% ggplot(aes(y=Count, x=Secchi, color = TowDuration)) + 
  geom_point()+ theme_bw()

TP_Salinity<-TPData_Binary %>% ggplot(aes(y=Count, x=Salinity, color = TowDuration)) + 
  geom_point() + theme_bw()
library(cowplot)
plot_grid(TP_Water, TP_DO, TP_Secchi, TP_Salinity, ncol = 2, labels = "AUTO")

```

From here on down, I am following Nima's "caroline_BRTs.rmd"

```{r}
gbmGrid <- expand.grid(interaction.depth = c(3,5,7), #tree complexity
                       n.trees = 2000, #number of trees
                       shrinkage = 0.1, # or seq(0.01, 0.5, length.out =5) #learning rate
                       #bag.fraction = 0.6, #bag fraction (proportion of observations used in selecting variables) and recommended by Elith
                       n.minobsinnode = 10 # the minimum number of observations in trees' terminal nodes. honestly dont worry about this.
                       )

gbmGrid %>% head() #take a look at what we just made
```

```{r}
fitControl <- trainControl(## 2-fold CV
                           method = "cv",
                           number = 2,
                           classProbs = TRUE, ## Estimate class probabilities
                           summaryFunction = twoClassSummary
                           )
```

Now we can use the `train` function to run the analysis.

I'll start with the most basic dataset for the most basic model: CPUE \~ WaterTemp + Secchi + DO + Salinity

```{r}
set.seed(124) #setting the seed for reproducibility. BRT have randomness in them so if you want to make sure you are getting the same result each time make sure to set the seed

gbmFit <- train(PresAbs ~ WaterTemperature + Secchi + DO + Salinity + TowDuration,
                data = TPData_Binary, 
                method = "gbm", 
                trControl = fitControl, 
                verbose = FALSE, 
                ## Now specify the exact models 
                ## to evaluate:
                tuneGrid = gbmGrid,
                metric = "ROC" # Not sure if this is best. Other options according to ChatGPT + help: RMSE, Rsquared MAE, MSE, MPE, MAPE, SMAPE
                )

gbmFit

# we can plot the results too which will be easier to interpret
ggplot(gbmFit)
```

train help file: "A string that specifies what summary metric will be used to select the optimal model. By default, possible values are "RMSE" and "Rsquared" for regression and "Accuracy" and "Kappa" for classification. If custom performance metrics are used (via the summaryFunction argument in trainControl, the value of metric should match one of the arguments. If it does not, a warning is issued and the first metric given by the summaryFunction is used. (NOTE: If given, this argument must be named.)"

From ChatGPT on choosing metric for train(): "Selecting the best metric for evaluating the performance of a regression model depends on the specific goals and characteristics of your predictive modeling problem. There isn't a one-size-fits-all answer, and the choice of metric should align with the objectives and requirements of your project. Here's a general guideline to help you decide which metric is best:

Mean Absolute Error (MAE):

Use MAE when you want to understand the average magnitude of errors in your predictions. MAE is less sensitive to outliers compared to MSE, which can make it a good choice when your dataset contains outliers. Mean Squared Error (MSE) and Root Mean Squared Error (RMSE):

MSE and RMSE give higher weight to larger errors, making them suitable when large errors are more costly or need to be minimized. RMSE is in the same unit as the target variable, which makes it easier to interpret compared to MSE. R-squared (R²):

R² measures the proportion of variance in the target variable that is explained by the model. A higher R² indicates a better fit. R² is a good choice when you want to understand how well your predictors explain the variability in the target variable. Adjusted R-squared (Adjusted R²):

Adjusted R² adjusts for the number of predictors in the model. It penalizes adding unnecessary predictors that do not improve the model's fit. It's useful when you want to balance model complexity with explanatory power. Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), and Symmetric Mean Absolute Percentage Error (SMAPE):

These metrics are used in cases where percentage errors are more meaningful than absolute errors. For instance, in forecasting and demand prediction."

```{r}
 set.seed(124) # for reproducibility 
 brt1 <- dismo::gbm.step(data=TPData_Binary, 
                         gbm.x= c(2:6), # environmental variables
                         gbm.y= 1, # response variable
                         family = "gaussian", # for counts this would be "poisson"
                         tree.complexity = 3, # complexity of the interactions that the model will fit
                         learning.rate = 0.15,  # optimized to end up with >1000 trees
                         bag.fraction = 0.6 # recommended by Elith, amount of input data used each time
                         )

# make sure to save the model!
 summary(brt1)
 # reponse curves
par(mar=c(2, 2, 1, 1))

dismo::gbm.plot(brt1, n.plots = 5, write.title= T, rug = T, smooth = TRUE, plot.layout=c(2,3), common.scale = T)
summary(TPData_Binary)
```
