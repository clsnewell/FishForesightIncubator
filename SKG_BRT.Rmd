---
title: "Modeling Protocol"
author: "Caroline Newell"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

Constructed so I can search "ShokihaziGoby"and replace with Gensp name and have the code run.

### Loading libraries and data

```{r}
library(readr)
library(dismo)
library(caret)
library(here)
library(cowplot)
library(Hmisc)
library(tidyverse)

PresAbs<-read_csv("C:/Users/cnewe/OneDrive/Documents/Incubator/Code/FishForesightIncubator/Data/SMFS_PresAbs.csv")
unique(PresAbs$gensp)
```

# PresenceAbsence

## Step 1: Data visualization, Quality Check

Before modeling, we will ensure the data is up to snuff.

First things first, we need to get the data.

```{r}
ShokihaziGobyData<-PresAbs %>% filter(gensp %in% "Tridentiger barbatus")
ShokihaziGobyData<-as.data.frame(ShokihaziGobyData)
```

Now lets check the dataset.

```{r}
glimpse(ShokihaziGobyData)
summary(ShokihaziGobyData)
```

Let's make a couple of plots for making predictions.

```{r}
Plot_Water<-ShokihaziGobyData %>% ggplot(aes(y=Binary, x=WaterTemperature, color = TowDuration)) + 
  geom_point()+ theme_bw()

Plot_DO<-ShokihaziGobyData %>% ggplot(aes(y=Binary, x=DO, color = TowDuration)) + 
  geom_point()+ theme_bw()

Plot_Secchi<-ShokihaziGobyData %>% ggplot(aes(y=Binary, x=Secchi, color = TowDuration)) + 
  geom_point()+ theme_bw()

Plot_Salinity<-ShokihaziGobyData %>% ggplot(aes(y=Binary, x=Salinity, color = TowDuration)) + 
  geom_point() + theme_bw()

plot_grid(Plot_Water, Plot_DO, Plot_Secchi, Plot_Salinity, ncol = 2, labels = "AUTO")

# I want to show presences vs. all samples.

Plot_Water<-ShokihaziGobyData %>% ggplot(aes(y=PresAbs, x=WaterTemperature)) + geom_violin(fill = "lightseagreen")+ stat_summary(fun.data="mean_sdl", fun.args=list(mult=1), geom= "pointrange", color="black") + theme_bw() + ylab("") + xlab("Water Temperature (C)") #+ facet_wrap(~Year)

Plot_DO<-ShokihaziGobyData %>% ggplot(aes(y=PresAbs, x=DO, color = TowDuration)) + 
  geom_violin(fill = "lightseagreen")+ stat_summary(fun.data="mean_sdl", fun.args=list(mult=1), geom= "pointrange", color="black") + theme_bw()+ ylab("")+ xlab("Dissolved Oxygen (mg/L)") #+ facet_wrap(~Year)

Plot_Secchi<-ShokihaziGobyData %>% ggplot(aes(y=PresAbs, x=Secchi)) + 
  geom_violin(fill = "lightseagreen")+ stat_summary(fun.data="mean_sdl", fun.args=list(mult=1), geom= "pointrange", color="black") + theme_bw()+ ylab("") + xlab("Secchi (cm)") #+ facet_wrap(~Year)

Plot_Salinity<-ShokihaziGobyData %>% ggplot(aes(y=PresAbs, x=Salinity)) + geom_violin(fill = "lightseagreen")+ stat_summary(fun.data="mean_sdl", fun.args=list(mult=1), geom= "pointrange", color="black") + theme_bw()+ ylab("") + xlab("Salinity") #+ facet_wrap(~Year)

Plots<-plot_grid(Plot_Water, Plot_DO, Plot_Secchi, Plot_Salinity, ncol = 2, labels = "AUTO")

# now add the title
title <- ggdraw() + draw_label("ShokihaziGoby presence/absence by water quality variables for SMFS 2011-2023", fontface = 'bold', x = 0, hjust = 0) +  theme(
    # add margin on the left of the drawing canvas,
    # so title is aligned with left edge of first plot
    plot.margin = margin(0, 0, 0, 7))

plot_grid(title, Plots, ncol = 1,
  # rel_heights values control vertical title margins
  rel_heights = c(0.1, 1))

SummTable <- ShokihaziGobyData %>% 
  group_by(PresAbs) %>% #break down yearly summaries by including Year as a grouping variable. 
  dplyr::summarize(
    Mean_WT = mean(WaterTemperature),
    SD_WT = sd(WaterTemperature),
    MinWT=min(WaterTemperature),
    MaxWT=max(WaterTemperature),
    Mean_DO = mean(DO),
    SD_DO = sd(DO),
    MinDO=min(DO),
    MaxDO=max(DO),
    Mean_Secchi = mean(Secchi),
    SD_Secchi = sd(Secchi),
    MinSecchi=min(Secchi),
    MaxSecchi=max(Secchi),
    Mean_Salinity = mean(Salinity),
    SD_Salinity = sd(Salinity),
    MinSal=min(Salinity),
    MaxSal=max(Salinity)
  ) 

print(SummTable)
write.csv(SummTable, here("data", "ShokihaziGoby_SummTable.csv"))

#Seasonality
ShokihaziGobyData$Year<-as.factor(ShokihaziGobyData$Year)

ShokihaziGobyData$Month<-as.factor(ShokihaziGobyData$Month)

ShokihaziGobyData$StationCode<-as.factor(ShokihaziGobyData$StationCode)

ShokihaziGobyData %>% summarize(TotalPresence=sum(Binary), sd_binary=sd(Binary), n=n(), PercentageOfPresence=100*(TotalPresence/n))

ShokihaziGobyData %>% group_by(Month) %>% summarize(TotalPresence=sum(Binary), sd_binary=sd(Binary), n=n(), PercentageOfPresence=100*(TotalPresence/n)) %>% ggplot(aes(x=Month, y=TotalPresence)) +geom_col()+geom_linerange(aes(ymin=TotalPresence-sd_binary, ymax=TotalPresence+sd_binary))+theme_bw(base_size = 26)+ylab("Trawls that catch ShokihaziGoby") +xlab("Month (summed 2011-2023)") #+ theme(axis.text.x = element_text(angle = 45, hjust=1))
ShokihaziGobyData %>% group_by(StationCode) %>% summarize(TotalPresence=sum(Binary), sd_binary=sd(Binary), n=n(), PercentageOfPresence=100*(TotalPresence/n)) %>% ggplot(aes(x=StationCode, y=PercentageOfPresence)) +geom_col()+theme_bw(base_size = 26)+ylab("% of trawls that catch ShokihaziGoby") +xlab("Station Code") + theme(axis.text.x = element_text(angle = 45, hjust=1))
```

Plot analysis:
Warmer water temperatures, DO constrained, Secchi below 70, all salinities.


## Step 2: Use gbm() from dismo to run model and create outputs

Thanks Nima and Elith!

```{r}
 set.seed(124) # for reproducibility 
#gbm.step wants bernoulli response {0, 1} 
 brt1 <- dismo::gbm.step(data=ShokihaziGobyData, 
                         gbm.x= c(21, 22, 23, 25), # environmental variables
                         gbm.y= 17, # response variable
                         family = "bernoulli", # for counts this would be "poisson"
                         tree.complexity = 4, # complexity of the interactions that the model will fit. Based on what caret told us.
                         learning.rate = 0.005,  # optimized to end up with >1000 trees
                         bag.fraction = 0.6 # recommended by Elith, amount of input data used each time
                         )

# make sure to save the model!
summary(brt1)

names(brt1) #tells us the components of output

# To pull out one component of the list, use a number (angaus.tc5.lr01[[29]]) or name (angaus.tc5.lr01$cv.statistics); but be careful - some are as big as the dataset! e.g. there will be 1000 fitted values - find this by typing length(angaus.tc5.lr01$fitted)

 # reponse curves
par(mfrow=c(3,2))

dismo::gbm.plot(brt1, n.plots = 5, write.title= T, rug = T, smooth = TRUE, plot.layout=c(2,3), common.scale = T)

gbm::plot.gbm(brt1, n.plots=5, return.grid=TRUE)

dismo::gbm.plot.fits(brt1)
#I assume these wtm's are weighted means and are the means used when calculating partial dependence? So each dot is a prediction made from a single tree?


# This code assesses the extent to which pairwise interactions exist in the data.

find.int <- gbm.interactions(brt1)
find.int
brt1$self.statistics$correlation[[1]]
# The returned object, here named test.int, is a list. The first 2 components summarise the results, first as a ranked list of the 5 most important pairwise interactions, and the second tabulating all pairwise interactions. The variable index numbers in $rank.list can be used for plotting.

# You can plot pairwise interactions like this:
par(mfrow=c(1,2))
gbm.perspec(brt1,1, 3) #3D PLOT of Water Temp and Salinity
gbm.perspec(brt1,2, 3) #salinity and DO
#5 is tow duration. 4 is secchi. 1 is water temp.2 is salinity. 3 is DO

# save the model
saveRDS(brt1, here("data", "ShokihaziGoby_brt_gbm_step.rds"))

```

lr = 0.005
tc = 4
bf = 0.6


fitting final gbm model with a fixed number of 1350 trees for Binary

mean total deviance = 0.583 
mean residual deviance = 0.439 
 
estimated cv deviance = 0.501 ; se = 0.006 
 
training data correlation = 0.465 
cv correlation =  0.309 ; se = 0.016 
 
training data AUC score = 0.856 
cv AUC score = 0.782 ; se = 0.011 
 
elapsed time -  0.52 minutes

$rank.list
  var1.index var1.names var2.index       var2.names int.size
1          3         DO          2         Salinity   107.40
2          3         DO          1 WaterTemperature   100.06

$interactions
                 WaterTemperature Salinity     DO Secchi
WaterTemperature                0     47.2 100.06  60.72
Salinity                        0      0.0 107.40   6.63
DO                              0      0.0   0.00   0.62
Secchi                          0      0.0   0.00   0.00

------------------------------------------------------------------------
lr = 0.005
tc = 3
bf = 0.6

fitting final gbm model with a fixed number of 1950 trees for Binary

mean total deviance = 0.583 
mean residual deviance = 0.441 
 
estimated cv deviance = 0.5 ; se = 0.012 
 
training data correlation = 0.459 
cv correlation =  0.31 ; se = 0.027 
 
training data AUC score = 0.853 
cv AUC score = 0.785 ; se = 0.016 
 
elapsed time -  0.47 minutes 

$rank.list
  var1.index var1.names var2.index       var2.names int.size
1          3         DO          2         Salinity   148.02
2          3         DO          1 WaterTemperature    83.36

$interactions
                 WaterTemperature Salinity     DO Secchi
WaterTemperature                0    34.11  83.36  60.75
Salinity                        0     0.00 148.02   8.91
DO                              0     0.00   0.00   0.27
Secchi                          0     0.00   0.00   0.00

Firstly, the things you can see: The R console will show some results (see the Word Working Guide of the Elith tutorial for an example). It reports a brief model summary & all the values are also retained in the model object, so they will be permanently kept (as long as you save the R workspace before quitting).

There will also be a graph.. This model was built with the default 10-fold cross-validation (CV). The solid black curve is the mean, and the dotted curves +- 1 standard error, for the changes in predictive deviance (ie as measured on the excluded folds of the CV). The red line shows the minimum of the mean, and the green line the number of trees at which that occurs. The final model that is returned in the model object is built on the full data set, using the number of trees identified as optimal.

## Validation

Below are 2 validation schemes that we typically use to assess how well the model is predicting. The first is a K-fold cross-validation where the data is split in k number of equal sections (i.e. folds). A model is the trained on one fold and then its predicts to the rest of the folds. It then iterates this process through all the folds. Most studies do 10 folds as this has shown to be a good number. Too low or too high has shown to potentially bias / cause variability in results.

Performance metrics that are calculated are deviance explained, AUC, and TSS. Deviance explained is a metric for how much variance did our model explain (you can think of is as R squared) and AUC and TSS are assessing predictive skill. Since you are using abundance data you may need to change that to RMSE or MAE (I’d suggest taking a look at the Metrics package for that)

NOTE: whether you are using gbm.fixed or gbm.step to fit your BRTS, you will have to make a slight change to this function below. I’ve commented lines out since I made the model above using gbm.fixed but if used gbm.step I would used the code on those lines instead.

```{r}
#k fold cross validation function

eval_kfold_brt <- function(dataInput, gbm.x, gbm.y, learning.rate = 0.005, k_folds = 10, tree.complexity = 4, bag.fraction = 0.6){
  dataInput$Kset <- dismo::kfold(dataInput, k_folds) #randomly allocate k groups
  Evaluations_kfold_BRT <- as.data.frame(matrix(data=0,nrow=5,ncol=4)) 
  colnames(Evaluations_kfold_BRT) <- c("k","Deviance","AUC","TSS")
  counter=1
  for (k in 1:k_folds){
    print(k)
    train <- dataInput[dataInput$Kset!=k,]
    test <- dataInput[dataInput$Kset==k,]
    
     brt.k <- dismo::gbm.step(data=train, gbm.x= gbm.x, 
                             gbm.y = gbm.y, 
                              family="bernoulli", 
                             tree.complexity = tree.complexity,
                             learning.rate = learning.rate, 
                             bag.fraction = bag.fraction)
    
    preds <- gbm::predict.gbm(brt.k, test,
                              n.trees=brt.k$gbm.call$best.trees, 
                              type="response")
    
    dev_eval3<-function(x){
      null <- x$self.statistics$mean.null #use with gbm.step
      res <- x$self.statistics$mean.resid #use with gbm.step
      dev=((null - res)/null)*100
      return(dev)
    }
    dev<-dev_eval3(brt.k)
    d <- cbind(test[gbm.y], preds)
    pres <- as.numeric(d[d[,1]==1,2])
    abs <- as.numeric(d[d[,1]==0,2])
    e <- dismo::evaluate(p=pres, a=abs)
    Evaluations_kfold_BRT[counter,1] <- k
    Evaluations_kfold_BRT[counter,2] <- dev
    Evaluations_kfold_BRT[counter,3] <- e@auc
    Evaluations_kfold_BRT[counter,4] <- max(e@TPR + e@TNR-1)
    counter=counter+1 
  }
  return(Evaluations_kfold_BRT)
}
  
  
brt_SDM_k <- eval_kfold_brt(dataInput = ShokihaziGobyData,
                        gbm.x = c(21, 22, 23, 25),
                        gbm.y=17, learning.rate = 0.005,
                        bag.fraction = 0.6, tree.complexity = 4,
                        k_folds = 10)
brt_SDM_k
```

The other validation scheme that we do is a Leave one [year] out cross-validation. Just like 5 folds but now were trained the model on all the data but one year. And then we predict to that one year that was not included. This give us a sense out how well can we predict year by year as there may be some environmental variability over time.

NOTE: Same as above of changing the code whether you are using gbm.fixed or gbm.step

```{r}
#leave one year out function

eval_loo_brt <- function(pres, abs, gbm.x, gbm.y, learning.rate = 0.005, tree.complexity = 4, bag.fraction = 0.6){
  
  pres$year <- pres$WaterYear
  abs$year <- abs$WaterYear
  
  ## setup output df
  Evaluations_LOO_BRT <- as.data.frame(matrix(data = 0, nrow = 1, ncol = 5))
  colnames(Evaluations_LOO_BRT) <- c("k","Deviance","AUC","TSS","n_pres")
  counter=1
  
  u_years <- unique(pres$year)
  for (y in u_years){
    
    print(paste('Running ', which(u_years == y), ' of ', length(u_years), sep=''), sep='')
    
    pres_train <- pres[which(pres$year != y),]
    ## absences are sampled from the full input absence df
    abs_train <- abs[sample(which(abs$year != y), size = nrow(pres_train), replace=F),]
    train <- rbind(pres_train, abs_train)
    
    pres_test <- pres[which(pres$year == y),]
    abs_test <- abs[sample(which(abs$year == y), size = nrow(pres_test), replace=F),]
    test <- rbind(pres_test, abs_test)
    
    
     brt.loo <- dismo::gbm.step(data=train, gbm.x= gbm.x, 
                                gbm.y = gbm.y, 
                                family="bernoulli", 
                                tree.complexity = tree.complexity,
                               learning.rate = learning.rate, 
                               bag.fraction = bag.fraction)
    
    ## make predictions for eval
    preds <- gbm::predict.gbm(brt.loo, test,
                              n.trees=brt.loo$gbm.call$best.trees, 
                              type="response")
    
    dev_eval3<-function(x){
      null <- x$self.statistics$mean.null #use with gbm.step
      res <- x$self.statistics$mean.resid #use with gbm.step
      dev=((null - res)/null)*100
      return(dev)
    }
    dev<-dev_eval3(brt.loo)
    d <- cbind(test[,gbm.y], preds)
    pres_y <- as.numeric(d[d[,1] == 1,2])
    abs_y <- as.numeric(d[d[,1] == 0,2])
    e <- dismo::evaluate(p = pres_y, a = abs_y)
    
    Evaluations_LOO_BRT[counter,1] <- y
    Evaluations_LOO_BRT[counter,2] <- dev
    Evaluations_LOO_BRT[counter,3] <- e@auc
    Evaluations_LOO_BRT[counter,4] <- max(e@TPR + e@TNR-1)
    Evaluations_LOO_BRT[counter,5] <- length(which(train[,gbm.y] == 1))
    
    counter=counter+1 
  }
  return(Evaluations_LOO_BRT)
}

brt_SDM_loo <- eval_loo_brt(pres = ShokihaziGobyData[which(ShokihaziGobyData$Binary == 1),],
                        abs = ShokihaziGobyData[which(ShokihaziGobyData$Binary == 0),],
                        gbm.x = c(21, 22, 23, 25),
                        gbm.y=17, learning.rate = 0.005,
                        bag.fraction = 0.6, tree.complexity = 4)
brt_SDM_loo

#combining the model output, and both validation results into a list to save
eval <- list(brt = brt1, brt_k = brt_SDM_k, brt_loo = brt_SDM_loo)

eval
#now save it

saveRDS(eval, here("data","ShokihaziGoby_brt_SDM_eval_gbm_step.rds"))
```

## Analyzing Response Curves & Variable Importance

Look at Nima's code for pretty plots: <https://github.com/nfarchadi/heatwave_impacts_on_fisheries/blob/main/scripts/4_plots/FS3_FS4_response_relimport_plots.R>

```{r}
# Fig. S3. The relative importance of drivers in distribution models
# Fig. S4. Response curves of each environmental variable from distribution models


###----response plots----####

brt_ShokihaziGoby <- here("data","ShokihaziGoby_brt_SDM_eval_gbm_step.rds") %>% readRDS()


# get the matrix out of the `plot.gbm`
var<-brt_ShokihaziGoby$brt$var.names
response_data<-data.frame()

for (i in 1:length(var)){
response_plot_data <- gbm::plot.gbm(brt_ShokihaziGoby$brt,
                                 i.var = var[i],
                                 return.grid = TRUE)

response_plot_data<-response_plot_data %>% gather("variable","x",-y)

response_data<-rbind(response_data, response_plot_data)

}

#Get it on the same scale as elith
#response_data$y <- response_data[,2] - mean(response_data[,2])
#response_data$y<-1/(1+exp(-response_data$y))

#response_data$y<-scale(response_data$y, scale = FALSE)

#Add a smoothed line to an active plot
#loess(y ~ x, span = 0.3)

response_data %>%
  ggplot() + 
  geom_line(aes(x=x, y=y), color = "lightseagreen", size =1) + 
  facet_wrap(~variable, scales = "free", nrow = 5) +
  labs(x = "Variable Values",
       y = "Marginal Effect")+
  theme_bw() + 
  theme(panel.spacing = unit(.30, "lines"),
        strip.text = element_text(size=10),
        strip.background = element_blank(),
        strip.text.x = element_text(margin = margin(0,0,.05,0, "cm")))
  
ggsave(here("Plots","LongfinSmelt_response_plots.png"),
       width = 7, height = 6, units = "in", dpi = 300)
#ggsave(here("Plots","LongfinSmelt_response_plots.svg"),
#       width = 7, height = 6, units = "in", dpi = 300)

###----variable contribution----####
rel.inf<-data.frame(Variable = brt_ShokihaziGoby$brt$contributions$var,
                        rel.inf=brt_ShokihaziGoby$brt$contributions$rel.inf)# %>% 



rel.inf %>% 
  ggplot()+
  geom_bar(aes(x = reorder(Variable, -rel.inf),y=rel.inf), fill = "lightseagreen",
           stat="identity", position=position_dodge())+
  theme_bw() +
  labs(x = "Environmental Variable",
       y = "Relative Importance (%)")+
  theme(axis.text.x = element_text(angle = 45,hjust=1))

ggsave(here("Plots","ShokihaziGoby_relative_importance.png"),
       width = 7, height = 6, units = "in", dpi = 300)
#ggsave(here("Plots","FS3_relative_importance.svg"),
#       width = 7, height = 6, units = "in", dpi = 300)
```

# NOTES

-   If number of trees below 1k, need to lower learning rate.
-   There may be a difference in the caret output and gbm output (5%). To avoid this can use the default grid to optimize parameters and use predict to have the same results... Or don't worry about it.
