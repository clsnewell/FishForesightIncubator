---
title: "BRT_SMFS"
author: "Caroline Newell"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading Libraries
```{r}
library(dismo)
library(tidyverse)
library(lubridate)
library(caret)
library(sf)
library(readr)
setwd("C:/Users/cnewe/OneDrive/Documents/Incubator/Code/FishForesightIncubator")
```

# Loading Data
```{r}
SMFS_OTR_Thesis_WithZeros <- read_csv("Data/SMFS_OTR_Thesis_WithZeros.csv")

#Next step: I need a CPUE column. Calculate as catch per minute trawl.
SMFS_OTR_Thesis_WithZeros$CPUE<-SMFS_OTR_Thesis_WithZeros$Count / SMFS_OTR_Thesis_WithZeros$TowDuration

summary(SMFS_OTR_Thesis_WithZeros$CPUE)
```

# TP Practice
```{r}
glimpse(SMFS_OTR_Thesis_WithZeros)
#Lets build a df with just TP data.
TPDataRaw<-SMFS_OTR_Thesis_WithZeros %>% filter(OrganismCode %in% "TP")
#TPData$CPUE<-as.numeric(TPData$CPUE)
glimpse(TPDataRaw)
summary(TPDataRaw)
```

7,588 trawls in this dataset.

I want to simplify the dataframe a bit to just include columns of interest: CPUE, WaterTemperature, Secchi, DO, Salinity

There are lots of columns I am interested in. For now I only want the bare minimum just to get a feel for these BRTs. 

```{r}
TPData<- TPDataRaw %>% select(c(CPUE, WaterTemperature, Secchi, DO, Salinity))

glimpse(TPData)


TPData$CPUE<-as.numeric(TPData$CPUE)
TPData$WaterTemperature<-as.numeric(TPData$WaterTemperature)
TPData$Secchi<-as.numeric(TPData$Secchi)
TPData$DO<-as.numeric(TPData$DO)
TPData$Salinity<-as.numeric(TPData$Salinity)
glimpse(TPData)

```


From here on down, I am following Nima's "caroline_BRTs.rmd" 

```{r}
gbmGrid <- expand.grid(interaction.depth = c(3,5,7), #tree complexity
                       n.trees = 2000, #number of trees
                       shrinkage = 0.1, # or seq(0.01, 0.5, length.out =5) #learning rate
                       #bag.fraction = 0.6, #bag fraction (proportion of observations used in selecting variables) and recommended by Elith
                       n.minobsinnode = 10 # the minimum number of observations in trees' terminal nodes. honestly dont worry about this.
                       )

gbmGrid %>% head() #take a look at what we just made
```

```{r}
fitControl <- trainControl(## 2-fold CV
                           method = "cv",
                           number = 2,
                           classProbs = TRUE, ## Estimate class probabilities
                           summaryFunction = twoClassSummary
                           )
```

Now we can use the `train` function to run the analysis.

I'll start with the most basic dataset for the most basic model: CPUE ~ WaterTemp + Secchi + DO + Salinity

```{r}
set.seed(124) #setting the seed for reproducibility. BRT have randomness in them so if you want to make sure you are getting the same result each time make sure to set the seed

gbmFit <- train(CPUE ~ WaterTemperature + Secchi + DO + Salinity,
                data = TPData, 
                method = "gbm", 
                trControl = fitControl, 
                verbose = FALSE, 
                ## Now specify the exact models 
                ## to evaluate:
                tuneGrid = gbmGrid,
                metric = "ROC" # Not sure if this is best. Other options according to ChatGPT + help: RMSE, Rsquared MAE, MSE, MPE, MAPE, SMAPE
                )
gbmFit

# we can plot the results too which will be easier to interpret
ggplot(gbmFit)
```

train help file: "A string that specifies what summary metric will be used to select the optimal model. By default, possible values are "RMSE" and "Rsquared" for regression and "Accuracy" and "Kappa" for classification. If custom performance metrics are used (via the summaryFunction argument in trainControl, the value of metric should match one of the arguments. If it does not, a warning is issued and the first metric given by the summaryFunction is used. (NOTE: If given, this argument must be named.)"

From ChatGPT on choosing metric for train(): "Selecting the best metric for evaluating the performance of a regression model depends on the specific goals and characteristics of your predictive modeling problem. There isn't a one-size-fits-all answer, and the choice of metric should align with the objectives and requirements of your project. Here's a general guideline to help you decide which metric is best:

Mean Absolute Error (MAE):

Use MAE when you want to understand the average magnitude of errors in your predictions.
MAE is less sensitive to outliers compared to MSE, which can make it a good choice when your dataset contains outliers.
Mean Squared Error (MSE) and Root Mean Squared Error (RMSE):

MSE and RMSE give higher weight to larger errors, making them suitable when large errors are more costly or need to be minimized.
RMSE is in the same unit as the target variable, which makes it easier to interpret compared to MSE.
R-squared (R²):

R² measures the proportion of variance in the target variable that is explained by the model. A higher R² indicates a better fit.
R² is a good choice when you want to understand how well your predictors explain the variability in the target variable.
Adjusted R-squared (Adjusted R²):

Adjusted R² adjusts for the number of predictors in the model. It penalizes adding unnecessary predictors that do not improve the model's fit.
It's useful when you want to balance model complexity with explanatory power.
Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), and Symmetric Mean Absolute Percentage Error (SMAPE):

These metrics are used in cases where percentage errors are more meaningful than absolute errors. For instance, in forecasting and demand prediction."